{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e584ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from unstructured.documents.elements import Element,Text,Image,FigureCaption\n",
    "import pytesseract\n",
    "import base64\n",
    "from IPython.display import display, Image as IPImage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41586378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"C:/Users/002PMB744/Desktop/RAG-Langchain/RAG-Langchain/Data\"\n",
    "# data_path_1= \"RAG-Langchain/Data\"\n",
    "# LANGCHAIN_TRACING_V2 = os.environ['LANGCHAIN_TRACING_V2']\n",
    "# LANGCHAIN_ENDPOINT = os.environ[\"LANGCHAIN_ENDPOINT\"]\n",
    "# LANGCHAIN_API_KEY = os.environ['LANGCHAIN_API_KEY']\n",
    "# OPENAI_API_KEY= os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec1700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! response_format is not default parameter.\n",
      "                response_format was transferred to model_kwargs.\n",
      "                Please confirm that response_format is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! tools is not default parameter.\n",
      "                tools was transferred to model_kwargs.\n",
      "                Please confirm that tools is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! tool_choice is not default parameter.\n",
      "                tool_choice was transferred to model_kwargs.\n",
      "                Please confirm that tool_choice is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                temperature=0.7, # Controls randomness: higher values (e.g., 1.0) for more creative, lower (e.g., 0.2) for more deterministic\n",
    "                top_p=1.0, # Nucleus sampling: controls diversity by sampling from the most probable tokens\n",
    "                frequency_penalty=0.0, # Penalizes new tokens based on their existing frequency in the text so far\n",
    "                presence_penalty=0.0, # Penalizes new tokens based on whether they appear\n",
    "                logit_bias={}, # Modifies the likelihood of specific tokens appearing in the output\\\n",
    "                response_format={\"type\": \"text\"}, # Specifies the format of the output (e.g., \"text\", \"json_object\")\n",
    "                seed=None, # For reproducible outputs\n",
    "                tools=[], # List of tools the model can call\n",
    "                tool_choice=\"auto\", # Controls how the model uses tools\n",
    "                logprobs=False, # Whether to return log probabilities of output tokens\n",
    "                top_logprobs=None # Number of top log probabilities to return\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06e30c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Criticism of political leaders, including Narendra Modi, often stems from their statements or actions that some people perceive as misleading or false. Reasons behind such perceptions can include:\\n\\n1. **Political Strategy**: Politicians may exaggerate achievements or downplay failures to maintain support and bolster their public image.\\n\\n2. **Misinformation**: In an era of rapid information sharing, statements can be taken out of context or misrepresented.\\n\\n3. **Partisan Perspectives**: Supporters and critics often interpret statements differently based on their political allegiance.\\n\\n4. **Media Influence**: The way media outlets report on a leader's statements can shape public perception, leading some to believe that a leader is lying.\\n\\n5. **Complex Issues**: Political issues are often complex, and simplifications can lead to misunderstandings about what a leader actually said or meant.\\n\\nIt's important for citizens to critically evaluate information from various sources and consider multiple perspectives when forming opinions about political figures.\" additional_kwargs={'parsed': None, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 32, 'total_tokens': 225, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None} id='run-1ce352f6-6a28-41f4-a0c9-23ff58da3edc-0' usage_metadata={'input_tokens': 32, 'output_tokens': 193, 'total_tokens': 225, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"why do narendra modi lie so much?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1fe0b",
   "metadata": {},
   "source": [
    "## Extract the data\n",
    "\n",
    "Extract the elements of the PDF that we will be able to use in the retrieval process. These elements can be: Text, Images, Tables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7abe49f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     pdf_writer.write(out_file)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Partition the newly created smaller PDF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m raw_chunks = \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# filename=pdf_file_path,\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhi_res\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFigure\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_output_dir_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_characters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# defaults to 500\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombine_text_under_n_chars\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mAppData\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mRoaming\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mRelease-24.07.0-0\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mpoppler-24.07.0\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mLibrary\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured\\partition\\common\\metadata.py:161\u001b[39m, in \u001b[36mapply_metadata.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# unique-ify elements\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# instance).\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[39m, in \u001b[36madd_chunking_strategy.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[32m     77\u001b[39m call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured\\partition\\pdf.py:225\u001b[39m, in \u001b[36mpartition_pdf\u001b[39m\u001b[34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, detect_language_per_element, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m exactly_one(filename=filename, file=file)\n\u001b[32m    224\u001b[39m languages = check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages)\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetect_language_per_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdetect_language_per_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured\\partition\\pdf.py:340\u001b[39m, in \u001b[36mpartition_pdf_or_image\u001b[39m\u001b[34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, detect_language_per_element, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m    339\u001b[39m     warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     elements = \u001b[43m_partition_pdf_or_image_local\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspooled_to_bytes_io_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdf_text_extractable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_text_extractable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdfminer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;66;03m# NOTE(crag): do not call _process_uncategorized_text_elements here, because\u001b[39;00m\n\u001b[32m    365\u001b[39m     \u001b[38;5;66;03m# extracted elements (which are text blocks outside of OD-determined blocks)\u001b[39;00m\n\u001b[32m    366\u001b[39m     \u001b[38;5;66;03m# are likely not Titles and should not be identified as such.\u001b[39;00m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured\\partition\\pdf.py:647\u001b[39m, in \u001b[36m_partition_pdf_or_image_local\u001b[39m\u001b[34m(filename, file, is_image, infer_table_structure, include_page_breaks, languages, ocr_languages, ocr_mode, model_name, hi_res_model_name, pdf_image_dpi, metadata_last_modified, pdf_text_extractable, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, analysis, analyzed_image_output_dir_path, starting_page_number, extract_forms, form_extraction_skip_tables, pdf_hi_res_max_pages, password, pdfminer_config, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    644\u001b[39m skip_analysis_dump = env_config.ANALYSIS_DUMP_OD_SKIP\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     inferred_document_layout = \u001b[43mprocess_file_with_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     extracted_layout, layouts_links = (\n\u001b[32m    656\u001b[39m         process_file_with_pdfminer(\n\u001b[32m    657\u001b[39m             filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    663\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m ([], [])\n\u001b[32m    664\u001b[39m     )\n\u001b[32m    666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m analysis:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:391\u001b[39m, in \u001b[36mprocess_file_with_model\u001b[39m\u001b[34m(filename, model_name, is_image, fixed_layouts, pdf_image_dpi, password, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    383\u001b[39m layout = (\n\u001b[32m    384\u001b[39m     DocumentLayout.from_image_file(\n\u001b[32m    385\u001b[39m         filename,\n\u001b[32m    386\u001b[39m         detection_model=detection_model,\n\u001b[32m    387\u001b[39m         element_extraction_model=element_extraction_model,\n\u001b[32m    388\u001b[39m         **kwargs,\n\u001b[32m    389\u001b[39m     )\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_image\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDocumentLayout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement_extraction_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43melement_extraction_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m )\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m layout\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:78\u001b[39m, in \u001b[36mDocumentLayout.from_file\u001b[39m\u001b[34m(cls, filename, fixed_layouts, pdf_image_dpi, password, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (image_path, fixed_layout) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(image_paths, fixed_layouts)):\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# NOTE(robinson) - In the future, maybe we detect the page number and default\u001b[39;00m\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# to the index if it is not detected\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Image.open(image_path) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         page = \u001b[43mPageLayout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdocument_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfixed_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m         pages.append(page)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_pages(pages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:316\u001b[39m, in \u001b[36mPageLayout.from_image\u001b[39m\u001b[34m(cls, image, image_path, document_filename, number, detection_model, element_extraction_model, fixed_layout)\u001b[39m\n\u001b[32m    314\u001b[39m     page.get_elements_using_image_extraction()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fixed_layout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[43mpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_elements_with_detection_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     page.elements = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:201\u001b[39m, in \u001b[36mPageLayout.get_elements_with_detection_model\u001b[39m\u001b[34m(self, inplace)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# NOTE(mrobinson) - We'll want make this model inference step some kind of\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# remote call in the future.\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m inferred_layout: LayoutElements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m inferred_layout = \u001b[38;5;28mself\u001b[39m.detection_model.deduplicate_detected_elements(\n\u001b[32m    203\u001b[39m     inferred_layout,\n\u001b[32m    204\u001b[39m )\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\models\\unstructuredmodel.py:64\u001b[39m, in \u001b[36mUnstructuredObjectDetectionModel.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Image) -> LayoutElements:\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Inference using function call interface.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\models\\unstructuredmodel.py:45\u001b[39m, in \u001b[36mUnstructuredModel.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Any) -> Any:\n\u001b[32m     44\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Inference using function call interface.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\models\\yolox.py:69\u001b[39m, in \u001b[36mUnstructuredYoloXModel.predict\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict using YoloX model.\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28msuper\u001b[39m().predict(x)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\unstructured_inference\\models\\yolox.py:114\u001b[39m, in \u001b[36mUnstructuredYoloXModel.image_processing\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    111\u001b[39m session = \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    113\u001b[39m ort_inputs = {session.get_inputs()[\u001b[32m0\u001b[39m].name: img[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]}\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m output = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# TODO(benjamin): check for p6\u001b[39;00m\n\u001b[32m    116\u001b[39m predictions = demo_postprocess(output[\u001b[32m0\u001b[39m], input_shape, p6=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\002PMB744\\Desktop\\RAG-Langchain\\Env-RAG\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:287\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    285\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the PDF file and desired page numbers\n",
    "\n",
    "base_dir = data_path\n",
    "pdf_file_name = \"Hands On Machine Learning with Scikit Learn and TensorFlow.pdf\"\n",
    "# pdf_file_name = \"NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
    "input_pdf_path= f\"{base_dir}/{pdf_file_name}\"\n",
    "# input_pdf_path = \"path/to/your/document.pdf\"\n",
    "output_dir = \"./output_pages\"\n",
    "page_start = 275 +22\n",
    "page_end = 311 +22  # For pages 3, 4, and 5\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Split the PDF into a smaller file with only the desired pages\n",
    "output_pdf_path = os.path.join(output_dir, f\"pages_{page_start}_to_{page_end}.pdf\")\n",
    "output_path = \"./content/\"\n",
    "pdf_writer = PdfWriter()\n",
    "pdf_reader = PdfReader(input_pdf_path)\n",
    "\n",
    "for page_num in range(page_start - 1, page_end):\n",
    "    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "\n",
    "with open(output_pdf_path, \"wb\") as out_file:\n",
    "    pdf_writer.write(out_file)\n",
    "\n",
    "# Partition the newly created smaller PDF\n",
    "raw_chunks = partition_pdf(\n",
    "    filename=output_pdf_path,\n",
    "    # filename=pdf_file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    infer_table_structure=True,\n",
    "    extract_image_block_types=[\"Image\",\"Figure\",\"Table\"],\n",
    "    image_output_dir_path=output_path,\n",
    "    extract_image_block_to_payload=True,\n",
    "    chunking_strategy=None,\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,\n",
    "    poppler_path = r'C:\\AppData\\Roaming\\Release-24.07.0-0\\poppler-24.07.0\\Library\\bin'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26134d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.FigureCaption'>\",\n",
       " \"<class 'unstructured.documents.elements.Footer'>\",\n",
       " \"<class 'unstructured.documents.elements.Formula'>\",\n",
       " \"<class 'unstructured.documents.elements.Image'>\",\n",
       " \"<class 'unstructured.documents.elements.ListItem'>\",\n",
       " \"<class 'unstructured.documents.elements.NarrativeText'>\",\n",
       " \"<class 'unstructured.documents.elements.Table'>\",\n",
       " \"<class 'unstructured.documents.elements.Text'>\",\n",
       " \"<class 'unstructured.documents.elements.Title'>\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get 2 types of elements from the partition_pdf function\n",
    "set([str(type(el)) for el in raw_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb1bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_class_prob': 0.9302425384521484,\n",
       " 'coordinates': {'points': ((np.float64(216.6841583251953),\n",
       "    np.float64(911.1228637695312)),\n",
       "   (np.float64(216.6841583251953), np.float64(1016.1986111111111)),\n",
       "   (np.float64(1210.255126953125), np.float64(1016.1986111111111)),\n",
       "   (np.float64(1210.255126953125), np.float64(911.1228637695312))),\n",
       "  'system': 'PixelSpace',\n",
       "  'layout_width': 1400,\n",
       "  'layout_height': 1838},\n",
       " 'last_modified': '2025-11-08T18:15:39',\n",
       " 'filetype': 'application/pdf',\n",
       " 'languages': ['eng'],\n",
       " 'page_number': 1,\n",
       " 'file_directory': './output_pages',\n",
       " 'filename': 'pages_297_to_333.pdf',\n",
       " 'parent_id': '2f6c211ad95eaa30ac640d29deb4088f'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each CompositeElement containes a bunch of related elements.\n",
    "# This makes it easy to use these elements together in a RAG pipeline.\n",
    "chunk_3 = raw_chunks[3].to_dict()\n",
    "chunk_3['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "images = []\n",
    "\n",
    "for chunk in raw_chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"Text\" in str(type((chunk))):\n",
    "        texts.append(chunk)\n",
    "\n",
    "for idx, chunk in enumerate(raw_chunks):\n",
    "    if isinstance(chunk, Image):\n",
    "        # check idx+ 1 is a figure caption\n",
    "        if idx + 1 < len(raw_chunks) and isinstance(raw_chunks[idx + 1], FigureCaption):\n",
    "            caption = raw_chunks[idx + 1].text\n",
    "        else:\n",
    "            caption = None\n",
    "        images.append(\n",
    "            # \"index\": idx,\n",
    "            # \"image_text\": chunk.text,\n",
    "            # \"caption\": caption if caption else \"No Caption\",\n",
    "            chunk.metadata.image_base64,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "# model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5844c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unable to access external websites. Please provide the table or text for summarization.',\n",
       " 'Training deep neural networks (DNNs) with multiple layers, such as 10 layers containing hundreds of neurons, is essential for complex tasks like detecting various objects in high-resolution images, but it presents significant challenges.',\n",
       " 'The chapter addresses the vanishing gradients problem and its solutions, discusses various optimizers that enhance training speed for large models compared to standard Gradient Descent, and reviews several popular regularization techniques for large neural networks.',\n",
       " 'The text introduces tools that enable the training of very deep neural networks in the field of Deep Learning.',\n",
       " 'The backpropagation algorithm computes the error gradient by propagating from the output layer to the input layer, calculating the gradient of the cost function for each layer.',\n",
       " '275 is a numerical value that could represent various things depending on the context, such as a quantity, measurement, or identifier. Without additional context, its significance remains unclear.',\n",
       " 'The parameter in the network is updated using gradients through a Gradient Descent step.',\n",
       " 'The text discusses two main issues in training deep neural networks: the vanishing gradients problem, where gradients become too small for effective weight updates in lower layers, preventing convergence, and the exploding gradients problem, where gradients become excessively large, causing divergence, particularly in recurrent neural networks. Overall, deep networks often experience unstable gradients, leading to varying learning speeds across layers.',\n",
       " \"Significant progress in understanding the training difficulties of deep neural networks began around 2010, particularly due to the work of Glorot and Bengio. They identified issues with the logistic sigmoid activation function and the common weight initialization method, which led to increasing output variances in each layer, ultimately causing saturation in the top layers. The logistic function's mean of 0.5 exacerbates this problem compared to the hyperbolic tangent function, which performs better with a mean of 0.\",\n",
       " 'The logistic activation function saturates at 0 or 1 for large inputs, resulting in a derivative near 0. This leads to minimal gradient during backpropagation, causing dilution of the gradient through the network layers, leaving lower layers with insufficient gradient.',\n",
       " 'Glorot and Bengio propose a method to maintain proper signal flow in neural networks by ensuring the variance of outputs and gradients remains equal to that of inputs. This is achieved through a weight initialization strategy known as Xavier or Glorot initialization, which involves random initialization based on the number of input and output connections, addressing the challenge of signal decay or explosion during forward and backward propagation.',\n",
       " 'Xavier initialization is a technique used to set the initial weights of a neural network layer when using the logistic activation function, promoting better convergence by maintaining a balanced variance across layers.',\n",
       " 'Please provide the table or text chunk that you would like summarized.',\n",
       " 'Normal distribution characterized by a mean of 0 and a standard deviation of σ, which equals the sum of the number of inputs (ninputs) and outputs (noutputs).',\n",
       " 'Please provide the table or text chunk you would like summarized.',\n",
       " 'The distribution is uniform between -r and +r, where r equals the sum of the number of inputs and outputs (r = ninputs + noutputs).',\n",
       " 'Simpler equations arise when the number of input connections equals the number of output connections, such as σ = 1/ninputs or r = 3/ninputs, as demonstrated in Chapter 10.3.',\n",
       " 'Xavier initialization accelerates training and contributes to the success of Deep Learning. Recent papers suggest similar strategies for various activation functions. The initialization for ReLU and its variants is known as He initialization.',\n",
       " 'Table 11-1 outlines the initialization parameters specific to various activation functions used in neural networks.',\n",
       " 'The activation function is a mathematical operation applied to neural network nodes, determining the output based on input signals. It introduces non-linearity, enabling the network to learn complex patterns. Common types include Sigmoid, Tanh, ReLU, and Softmax, each with different properties and applications in machine learning models.',\n",
       " 'Logistics involves the planning, implementation, and coordination of the movement and storage of goods, services, and information within a supply chain, ensuring efficiency and timely delivery.',\n",
       " 'The hyperbolic tangent is a mathematical function denoted as tanh(x), representing the ratio of the hyperbolic sine and cosine functions. It is used in various fields, including mathematics, physics, and engineering, and has properties similar to the regular tangent function, including asymptotic behavior and periodicity. The function ranges between -1 and 1 and is commonly used in neural networks and other applications requiring non-linear transformations.',\n",
       " 'ReLU (Rectified Linear Unit) is an activation function used in neural networks that outputs the input directly if it is positive, and zero otherwise. Variants include Leaky ReLU, which allows a small, non-zero gradient when the input is negative, and Parametric ReLU, where the slope for negative inputs is learned during training. Other variants like Exponential Linear Unit (ELU) and Scaled Exponential Linear Unit (SELU) aim to improve training speed and model performance by addressing issues like dying neurons and variance.',\n",
       " 'The fully_connected() function uses Xavier initialization by default, but it can be changed to He initialization using the variance_scaling_initializer() function.',\n",
       " 'The code snippet initializes a TensorFlow variable with a variance scaling initializer from the `tf.contrib.layers` module.',\n",
       " 'The code snippet defines a fully connected layer in a neural network with input `X`, a specified number of hidden units `n_hidden1`, using He initialization for weights, and assigns it to the scope \"h1\".',\n",
       " 'He initialization focuses solely on fan-in, unlike Xavier initialization, which averages fan-in and fan-out. This approach is the default for the variance_scaling_initializer() function, but it can be altered by setting mode=\"FAN_AVG\".',\n",
       " 'The 2010 paper by Glorot and Bengio revealed that vanishing/exploding gradient problems in deep neural networks were partly due to poor activation function choices. While sigmoid functions were commonly used, the ReLU activation function is more effective as it does not saturate for positive values and is computationally efficient.',\n",
       " 'The ReLU activation function has a limitation known as \"dying ReLUs,\" where some neurons stop producing output and remain at 0 during training, particularly with high learning rates. This can lead to a significant portion of the network\\'s neurons becoming inactive. Once a neuron outputs 0, it is unlikely to recover because the gradient of the ReLU function is also 0 for negative inputs.',\n",
       " 'The leaky ReLU function, defined as LeakyReLUα(z) = max(αz, z), addresses the \"dying ReLU\" problem by allowing a small slope (α) for negative values, typically set to 0.01. Research shows that leaky variants outperform the strict ReLU, with α = 0.2 yielding better results than α = 0.01. The randomized leaky ReLU (RReLU) and parametric leaky ReLU (PReLU), where α is either randomly chosen during training or learned via backpropagation, also demonstrate effective performance and help reduce overfitting.',\n",
       " 'The method outperforms ReLU on large image datasets but may overfit on smaller datasets.',\n",
       " 'The 2015 paper by Djork-Arné Clevert et al. introduced the exponential linear unit (ELU) activation function, which demonstrated superior performance compared to various ReLU variants, resulting in reduced training time and improved test set performance.',\n",
       " 'The ELU (Exponential Linear Unit) activation function is defined by Equation 11-2, which enhances the learning characteristics by allowing for negative values, thus addressing the vanishing gradient problem. It combines exponential decay for negative inputs and a linear output for positive inputs.',\n",
       " 'The ELUα function is defined as α times the exponential of z minus 1 for z less than 0, and as z for z greater than or equal to 0.',\n",
       " 'The discussed function resembles the ReLU function but has several significant differences.',\n",
       " 'The ELU activation function is slower to compute than ReLU and its variants due to the exponential function, but it offers faster convergence during training. However, at test time, ELU networks are slower than ReLU networks.',\n",
       " 'For hidden layers in deep neural networks, the preferred activation functions generally rank as follows: ELU > leaky ReLU > ReLU > tanh > logistic. For better runtime performance, leaky ReLUs may be favored over ELUs. Default α values are recommended (0.01 for leaky ReLU and 1 for ELU). Cross-validation can be used to explore other functions like RReLU for overfitting or PReLU for large training sets.',\n",
       " 'TensorFlow provides an elu() function for neural network construction, which can be utilized by setting the activation_fn argument in the fully_connected() function.',\n",
       " 'The code defines a fully connected layer in a neural network with input X, a specified number of hidden units (n_hidden1), and uses the ELU activation function.',\n",
       " 'TensorFlow lacks a built-in function for leaky ReLUs, but users can easily define one.',\n",
       " 'The function `leaky_relu` defines a leaky ReLU activation function, which allows a small, non-zero gradient when the input is negative, helping to prevent the \"dying ReLU\" problem. It accepts an input `z` and an optional parameter `name`.',\n",
       " 'The expression returns the maximum value between 0.01 times z and z itself, effectively applying a threshold to z to ensure it is not less than 0.01 times its value.',\n",
       " 'The table describes a fully connected neural network layer that uses the leaky ReLU activation function, with input X and a specified number of hidden units n_hidden1.',\n",
       " 'The content from finelybook.com is not accessible for summarization.',\n",
       " \"Using He initialization with ELU or variants of ReLU can reduce vanishing/exploding gradient problems initially, but it does not ensure they won't reoccur during training.\",\n",
       " 'In 2015, Sergey Ioffe and Christian Szegedy introduced Batch Normalization (BN) to tackle the vanishing/exploding gradients issues and the Internal Covariate Shift problem caused by changing input distributions during training.',\n",
       " 'The technique involves adding an operation before the activation function in each layer to zero-center and normalize the inputs, followed by scaling and shifting using two new parameters per layer, allowing the model to learn the optimal scale and mean of the inputs.',\n",
       " 'The algorithm uses the mean and standard deviation of the current mini-batch to zero-center and normalize the inputs, a process known as Batch Normalization.',\n",
       " 'The Batch Normalization algorithm (Equation 11-3) normalizes the inputs of each layer in a neural network by adjusting and scaling the activations. It computes the mean and variance of the inputs, normalizes them, and applies learnable parameters for scaling and shifting to improve training stability and speed.',\n",
       " 'σB represents the empirical standard deviation calculated across the entire mini-batch.',\n",
       " 'Unable to access external websites or content. Please provide the table or text for summarization.',\n",
       " \"During testing, the whole training set's mean and standard deviation are used for batch normalization instead of a mini-batch. Four parameters are learned for each batch-normalized layer: γ (scale), β (offset), μ (mean), and σ (standard deviation).\",\n",
       " 'The technique significantly improved deep neural networks by reducing the vanishing gradients problem, allowing the use of saturating activation functions and larger learning rates, which accelerated the learning process. Specifically, Batch Normalization achieved the same accuracy as a state-of-the-art image classification model with 14 times fewer training steps and improved results on ImageNet classification, reaching 4.9% top-5 validation error. Additionally, it functioned as a regularizer, minimizing the need for other regularization methods.',\n",
       " 'Batch Normalization increases model complexity and slows down prediction times due to additional computations, which may not be ideal for applications requiring rapid predictions.',\n",
       " 'It seems that there is no table or text chunk provided. Please provide the content that needs to be summarized.',\n",
       " 'Training may initially be slow as Gradient Descent searches for optimal scales and offsets for each layer, but it speeds up once good values are found.',\n",
       " 'Batch normalization is a technique used in deep learning to stabilize and accelerate the training of neural networks by normalizing the inputs of each layer. In TensorFlow, it can be implemented using the `tf.keras.layers.BatchNormalization` layer, which adjusts the mean and variance of the inputs during training and applies learned parameters during inference. This helps in reducing internal covariate shift and can improve model performance.',\n",
       " 'TensorFlow offers a batch_normalization() function that requires manual computation of mean and standard deviation, along with scaling and offset parameters. A more convenient option is the batch_norm() function, which automates these tasks and can be integrated with the fully_connected() function.',\n",
       " 'No information provided to summarize.',\n",
       " \"The code imports the TensorFlow library and the batch normalization function from TensorFlow's contrib.layers module.\",\n",
       " 'The neural network configuration includes an input layer with 784 nodes (28x28), two hidden layers with 300 and 100 nodes respectively, and an output layer with 10 nodes.',\n",
       " 'Defines a TensorFlow placeholder named \"X\" for input data with a shape that can accommodate any number of samples and a fixed number of features represented by `n_inputs`.',\n",
       " 'The code defines a TensorFlow placeholder for a boolean variable `is_training` and creates a dictionary `bn_params` containing parameters for batch normalization, including `is_training`, a decay rate of 0.99, and no updates collection.',\n",
       " 'The code snippet defines a fully connected neural network layer named \"hidden1\" with input \"X\" and \"n_hidden1\" as the number of hidden units.',\n",
       " 'The table defines a batch normalization function with specified parameters for normalization.',\n",
       " 'The operation defines a fully connected layer named \"hidden2\" that takes \"hidden1\" as input and has \"n_hidden2\" units.',\n",
       " 'The text refers to a batch normalization function (`batch_norm`) and its associated parameters (`bn_params`).',\n",
       " 'The code snippet defines a fully connected layer named \"outputs\" with a specified number of outputs (`n_outputs`) and applies batch normalization using given parameters (`bn_params`) without an activation function.',\n",
       " \"The code defines an `is_training` placeholder that indicates whether the batch normalization function should use the current mini-batch's statistics during training or the running averages during testing.\",\n",
       " 'The bn_params dictionary defines parameters for the batch_norm() function, including is_training. The algorithm utilizes exponential decay for running averages, updating values with the formula v = v × decay + v × (1 - decay). A suitable decay value is usually close to 1, like 0.9, 0.99, or 0.999, with more nines preferred for larger datasets.',\n",
       " \"The `updates_collections` parameter in TensorFlow's `batch_norm()` function should be set to None to update running averages during training when `is_training=True`. If not set, TensorFlow defaults to adding operations for updating running averages to a collection that must be executed manually.\",\n",
       " 'The layers are created using the fully_connected() function, incorporating batch_norm() for input normalization before the activation function, as done in Chapter 10.',\n",
       " 'By default, batch_norm() centers, normalizes, and shifts inputs without scaling (γ is fixed to 1), which is suitable for layers with no activation or ReLU. For other activation functions, \"scale\": True should be added to bn_params.',\n",
       " 'To reduce repetition in defining multiple layers with identical parameters, the arg_scope() function can be used. This function takes a list of functions and applies the specified parameters to them automatically.',\n",
       " 'It appears that there is no specific table or text provided for summarization. Please provide the content you would like summarized.',\n",
       " 'Please provide the table or text chunk that you would like summarized.',\n",
       " \"The code snippet uses TensorFlow's `arg_scope` to apply batch normalization to fully connected layers. It defines three layers: two hidden layers (`hidden1` and `hidden2`) with specified neuron counts (`n_hidden1`, `n_hidden2`), and an output layer (`logits`) without an activation function.\",\n",
       " 'Using structured code for setting activation functions, initializers, normalizers, and regularizers in a model with multiple layers enhances readability.',\n",
       " 'The construction phase involves defining the cost function, creating an optimizer to minimize it, defining evaluation operations, and creating a Saver, similar to the process in Chapter 10.',\n",
       " 'In the execution phase, when running operations that depend on the batch_norm layer, the is_training placeholder must be set to either True or False.',\n",
       " 'The code initializes a TensorFlow session and executes the initialization operation.',\n",
       " 'The code iterates through a loop for a specified number of epochs, denoted by `n_epochs`.',\n",
       " 'Please provide the table or text chunk you would like summarized.',\n",
       " 'The code iterates through pairs of X_batches and y_batches simultaneously.',\n",
       " 'The code snippet indicates the execution of a training operation in a machine learning context using the `sess.run` method.',\n",
       " 'The code snippet initializes a feed dictionary for a training session, setting the training mode to true and providing a batch of input data (X_batch) and corresponding labels (y_batch).',\n",
       " 'The accuracy score is evaluated using the `accuracy.eval()` method.',\n",
       " 'The code snippet evaluates the accuracy score of a model using scaled test data (`X_test_scaled`) and corresponding labels (`y_test`) while indicating that the model is not in training mode.',\n",
       " 'Batch Normalization may have limited impact in shallow networks but can greatly benefit deeper networks.',\n",
       " 'Gradient Clipping is a technique used to mitigate the exploding gradients problem in backpropagation, particularly in recurrent neural networks. While Batch Normalization is generally preferred now, understanding Gradient Clipping and its implementation remains valuable.',\n",
       " 'In TensorFlow, to use an optimizer, first call compute_gradients() to compute gradients, then use clip_by_value() to clip the gradients, and finally apply the clipped gradients with apply_gradients().',\n",
       " 'The threshold is set to 1.0.',\n",
       " \"The code snippet demonstrates the use of TensorFlow's GradientDescentOptimizer to compute gradients of a loss function, clip the gradient values within a specified threshold, and apply the modified gradients to update the model's parameters.\",\n",
       " 'The training operation computes gradients at each step, clips them between -1.0 and 1.0, and applies them, with the clipping threshold being a tunable hyperparameter.',\n",
       " 'Training a very large deep neural network (DNN) from scratch is not advisable; it is better to find an existing neural network that performs a similar task.',\n",
       " 'The paper discusses the challenges in training recurrent neural networks (RNNs), particularly focusing on issues such as vanishing and exploding gradients, which hinder the learning process. It emphasizes the need for better training techniques and architectures to address these difficulties.',\n",
       " 'Transfer learning utilizes pre-trained network layers to enhance training speed and reduce the amount of required training data.',\n",
       " 'A DNN trained to classify 100 categories can be adapted to classify specific vehicle types by reusing parts of the original network due to the similarity of the tasks.',\n",
       " 'If the input pictures differ in size from those used in the original task, they must be resized to match the expected dimensions for effective transfer learning, which relies on similar low-level features in the inputs.',\n",
       " 'If the original model was trained with TensorFlow, it can be restored and retrained for a new task.',\n",
       " 'The provided text or table is missing, so a summary cannot be generated.',\n",
       " 'The code snippet demonstrates how to restore a TensorFlow model from a checkpoint file (\"./my_original_model.ckpt\") using a session, allowing for further training on a new task.',\n",
       " 'To reuse part of an original model, configure the Saver to restore only specific variables, such as hidden layers 1, 2, and 3.',\n",
       " 'Build a new model maintaining the same definition for hidden layers 1-3.',\n",
       " 'The code initializes all global variables in a TensorFlow session.',\n",
       " 'The code snippet retrieves all trainable variables from the TensorFlow graph using the `tf.get_collection` method with the `tf.GraphKeys.TRAINABLE_VARIABLES` key.',\n",
       " 'The provided input is not recognizable as a table or text chunk suitable for summarization.',\n",
       " 'The code creates a dictionary mapping the names of reusable variables to themselves and initializes a TensorFlow saver to restore the original model using these variables.',\n",
       " 'The code snippet initializes a TensorFlow Saver object named `new_saver` to save a new model.',\n",
       " 'The code snippet demonstrates the use of a TensorFlow session, indicating that operations will be executed within the context of that session.',\n",
       " 'The code initializes a session, restores layers 1 to 3 from an original model checkpoint, trains a new model, and saves the entire new model to a specified checkpoint.',\n",
       " 'The process involves building a new model by copying the hidden layers 1 to 3 from the original model, initializing variables, and filtering for trainable variables in those layers. A dictionary is created to map variable names from the original to the new model, and two Savers are established: one for restoring the copied variables and another for saving the entire new model. After initializing the session and restoring values, the model is trained on a new task and saved.',\n",
       " 'For similar tasks, reuse more layers, starting with lower ones; for very similar tasks, retain all hidden layers and only change the output layer.',\n",
       " 'To use weights from a model trained with a different framework, load the weights manually (e.g., with Theano) and assign them to the correct variables, which can be a tedious process. An example code snippet demonstrates copying weights and biases from the first hidden layer of such a model.',\n",
       " 'Weights and biases are being loaded from another framework.',\n",
       " 'The code snippet illustrates the creation of a TensorFlow placeholder for input data and defines a fully connected layer named \"hidden1\" with a specified number of hidden units.',\n",
       " 'The code snippet demonstrates how to access variables created by the `fully_connected()` function in TensorFlow using a root scope and variable reuse. It retrieves the weights and biases for the first hidden layer as `hidden1_weights` and `hidden1_biases`, respectively.',\n",
       " 'The code defines TensorFlow placeholders for weights and biases of a neural network, specifically for the first hidden layer, and includes assignments to set these values.',\n",
       " 'The code initializes all global variables in TensorFlow using the function `tf.global_variables_initializer()`.',\n",
       " 'The code snippet demonstrates the use of a TensorFlow session, indicating that operations will be executed within that session context.',\n",
       " 'The command `sess.run(init)` is used to initialize variables in a TensorFlow session.',\n",
       " \"The code snippet demonstrates how to assign original weights and biases to a neural network's hidden layer using TensorFlow's `sess.run()` function, followed by training the model on a new task.\",\n",
       " 'Lower layers of the first DNN likely detect useful low-level features for image classification tasks, and it is advisable to freeze their weights during training to facilitate the training of higher layers. To do this, the optimizer should be provided with a list of variables to train that excludes the lower layer variables.',\n",
       " 'The code snippet retrieves all trainable variables from a TensorFlow graph using `tf.get_collection` with the `tf.GraphKeys.TRAINABLE_VARIABLES` key.',\n",
       " 'The provided content is not visible or accessible for summarization. Please provide the relevant table or text for summarization.',\n",
       " 'The code snippet indicates that an optimizer is being used to minimize a loss function with respect to a list of training variables.',\n",
       " 'The text explains that only the trainable variables in hidden layers 3 and 4, along with the output layer, are provided to the optimizer, resulting in hidden layers 1 and 2 being frozen during training.',\n",
       " 'Unable to access external content, including the specified website. Please provide the text or table directly for summarization.',\n",
       " 'Caching the output of the topmost frozen layer for each training instance can significantly speed up training, as the frozen layers only need to be processed once per instance rather than once per epoch.',\n",
       " \"The code runs a session to compute the output of the 'hidden2' layer using the training data 'X_train'.\",\n",
       " 'During training, batches of outputs from hidden layer 2 are built instead of batches of training instances for the training operation.',\n",
       " 'The text imports the NumPy library in Python, which is commonly used for numerical computing and handling arrays.',\n",
       " 'The training process consists of 100 epochs and 500 batches.',\n",
       " 'The code snippet iterates through a range defined by `n_epochs`, executing the loop for each epoch.',\n",
       " 'The variable `hidden2_batches` contains the result of splitting the array `hidden2_outputs` (indexed by `shuffled_idx`) into `n_batches` equal parts.',\n",
       " 'The code snippet splits the `y_train` array, using shuffled indices, into `n_batches` equal parts.',\n",
       " 'The code snippet iterates over pairs of elements from two lists, `hidden2_batches` and `y_batches`, using a for loop.',\n",
       " 'The last line executes the training operation, freezing layers 1 and 2, and uses outputs from the second hidden layer along with their corresponding targets, allowing TensorFlow to avoid evaluating that layer and its dependencies.',\n",
       " 'The output layer of the original model typically needs to be replaced for new tasks, as it may not be useful or have the correct number of outputs.',\n",
       " 'The upper hidden layers of the original model are generally less useful for new tasks compared to the lower layers, as the high-level features relevant to the new task may differ significantly from those of the original task. It is important to determine the appropriate number of layers to reuse.',\n",
       " 'To optimize model performance, first freeze all copied layers and train the model. Then, unfreeze one or two top hidden layers to allow backpropagation adjustments, monitoring for performance improvements. More training data allows for unfreezing additional layers.',\n",
       " 'To improve performance with limited training data, consider dropping the top hidden layers and freezing the remaining layers.',\n",
       " 'Iterate to identify the optimal number of layers for reuse; if sufficient training data is available, consider replacing top hidden layers instead of dropping them, and potentially adding more hidden layers.',\n",
       " 'Neural networks for specific tasks can be found in personal model catalogs or in model zoos, where pretrained models are shared by others.',\n",
       " \"TensorFlow's model zoo at https://github.com/tensorflow/models includes state-of-the-art image classification networks like VGG, Inception, and ResNet, along with code, pretrained models, and tools for downloading popular models.\",\n",
       " 'Image datasets are collections of images used for training and evaluating machine learning models, typically categorized by specific criteria such as size, type, and application. They play a crucial role in computer vision tasks, including image recognition, classification, and segmentation.',\n",
       " \"Caffe's Model Zoo offers a variety of computer vision models, including LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet, and Inception, trained on datasets like ImageNet, Places Database, and CIFAR10. A converter by Saumitro Dasgupta is available on GitHub.\",\n",
       " 'In cases where labeled training data is scarce and no suitable pre-trained model exists, one can still utilize unsupervised pretraining with available unlabeled data. This involves training layers sequentially from the lowest to the highest using algorithms like Restricted Boltzmann Machines or autoencoders, freezing previously trained layers. After completing this process, the network can be fine-tuned with supervised learning through backpropagation.',\n",
       " 'Geoffrey Hinton and his team revived neural networks and Deep Learning in 2006 using a technique involving unsupervised pretraining, primarily with RBMs. This method was standard for deep nets until 2010, when the vanishing gradients problem was addressed, allowing for more common training of DNNs through backpropagation. Today, unsupervised pretraining, often using autoencoders, remains a viable option.',\n",
       " 'Reusing pretrained layers involves utilizing weights from previously trained models to enhance the performance and efficiency of new models. This technique can lead to improved generalization, reduced training time, and decreased resource consumption in various machine learning tasks.',\n",
       " 'No content provided for summarization.',\n",
       " '291',\n",
       " 'The task involves solving a complex problem without a comparable model, limited labeled training data, but an abundance of unlabeled training data.',\n",
       " 'Training a neural network on an auxiliary task with easily obtainable labeled data can help in reusing its lower layers for a different task, as these layers learn reusable feature detectors.',\n",
       " 'To effectively build a face recognition system with limited individual pictures, one could collect numerous images of random people online to train a neural network to identify whether two pictures depict the same person.',\n",
       " 'Unable to access external websites or content. Please provide the text or table you would like summarized.',\n",
       " 'Reusing the lower layers of a network that learns effective feature detectors for faces enables the training of a robust face classifier with minimal training data.',\n",
       " 'A cost-effective method for training neural networks involves initially labeling unlabeled examples as \"good\" and then generating corrupted instances labeled as \"bad.\" This approach allows for the training of a neural network to distinguish between good and bad examples, enhancing its understanding of language through the reuse of learned lower layers.',\n",
       " 'Please provide the table or text you would like summarized.',\n",
       " 'Max margin learning involves training a network to assign scores to training instances, ensuring that good instances have scores at least a specified margin greater than those of bad instances.',\n",
       " 'To speed up training of large deep neural networks, four strategies are recommended: good weight initialization, effective activation functions, Batch Normalization, and reusing pretrained network components. Additionally, using faster optimizers like Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and Adam can significantly enhance training speed.',\n",
       " 'Adam optimization is recommended for training due to its speed advantages over GradientDescentOptimizer. It has three tunable hyperparameters in addition to the learning rate, but default values generally suffice. Understanding its foundational concepts from other algorithms can be beneficial if adjustments are needed.',\n",
       " 'The text appears to reference a website, \"finelybook.com,\" likely indicating a platform for downloading content or resources. No specific details about the content or purpose of the site are provided.',\n",
       " 'Momentum optimization, proposed by Boris Polyak in 1964, allows for quicker convergence by gaining speed down a slope, unlike regular Gradient Descent, which progresses slowly with small steps.',\n",
       " 'Gradient Descent updates weights θ by subtracting the gradient of the cost function J(θ) multiplied by the learning rate η, following the equation θ ← θ – η∇θJ(θ). It does not consider previous gradients and may progress slowly if the local gradient is small.',\n",
       " 'Momentum optimization utilizes previous gradients by adding the local gradient to a momentum vector, which is then used to update weights by subtraction. This approach treats the gradient as an acceleration rather than a speed. To control momentum growth, a hyperparameter β (momentum) is introduced, typically set around 0.9, where values range from 0 (high friction) to 1 (no friction).',\n",
       " 'Equation 11-4 describes the momentum algorithm, which is a technique used in optimization to accelerate convergence by considering past gradients. It incorporates a momentum term that helps to smooth out updates, allowing for faster learning and improved performance in various optimization tasks.',\n",
       " 'The expression represents the update rule in optimization, combining a learning rate (β) with the gradient (∇θJ) of a cost function (J) with respect to parameters (θ) and a term (η) that may influence the update.',\n",
       " 'Momentum optimization achieves a terminal velocity that is 10 times the gradient multiplied by the learning rate when β = 0.9, allowing it to escape plateaus and navigate elongated cost functions more efficiently than Gradient Descent. This results in faster convergence, particularly in deep neural networks without Batch Normalization.',\n",
       " 'Momentum optimization aids in navigating different scales and overcoming local optima.',\n",
       " 'The optimizer may oscillate around the minimum due to momentum, overshooting and correcting multiple times. Introducing friction helps reduce these oscillations and accelerates convergence.',\n",
       " 'Momentum optimization in TensorFlow is easily implemented by substituting the GradientDescentOptimizer with the MomentumOptimizer.',\n",
       " 'The code snippet initializes a MomentumOptimizer in TensorFlow with a specified learning rate and a momentum value of 0.9.',\n",
       " 'Momentum optimization improves speed compared to Gradient Descent but requires tuning an additional hyperparameter, typically set to 0.9 for effective results.',\n",
       " 'Nesterov Momentum optimization, introduced by Yurii Nesterov in 1983, improves upon vanilla Momentum optimization by measuring the gradient of the cost function ahead in the direction of momentum, resulting in faster convergence.',\n",
       " 'Nesterov Accelerated Gradient (NAG) algorithm is a method for optimizing convex functions that combines momentum with a lookahead approach, improving convergence speed by utilizing gradient information from a preemptive position.',\n",
       " 'The expression represents a formula consisting of terms: β, η, the gradient of J with respect to θ (∇θJ), and an additional β.',\n",
       " 'The Nesterov update improves accuracy by using the gradient measured slightly further in the direction of the momentum vector, which generally points toward the optimum, rather than the gradient at the original position.',\n",
       " 'NAG optimization improves speed over regular Momentum by making small adjustments that accumulate over time, reducing oscillations and accelerating convergence, especially when navigating valleys in the weight landscape.',\n",
       " 'NAG generally accelerates training compared to standard Momentum optimization; enable it by setting use_nesterov=True in MomentumOptim.',\n",
       " 'The provided text appears to be incomplete or unclear, making it impossible to summarize. Please provide a complete table or text chunk for summarization.',\n",
       " 'The code defines a TensorFlow optimizer using the Momentum optimization algorithm with a specified learning rate, momentum of 0.9, and Nesterov acceleration enabled.',\n",
       " \"Gradient Descent initially descends quickly down steep slopes but slows near the valley's bottom; improving the algorithm to detect this and adjust direction towards the global optimum would be beneficial.\",\n",
       " 'The AdaGrad algorithm scales down the gradient vector in the steepest dimensions to optimize performance.',\n",
       " 'The AdaGrad algorithm adjusts the learning rate for each parameter based on the historical gradients, allowing for more frequent updates for infrequent features and diminishing updates for frequent features. It aims to improve convergence in optimization problems by adapting the learning rate dynamically.',\n",
       " 'The first step involves accumulating the squares of the gradients into a vector s, where each element si represents the sum of the squares of the partial derivatives of the cost function with respect to parameter θi. As the cost function becomes steeper along the ith dimension, the value of si increases with each iteration.',\n",
       " 'The second step resembles Gradient Descent but differs by scaling the gradient vector down by a factor of +, using element-wise division represented by ⊘ and a smoothing term ϵ to prevent division by zero, typically set to 10–10.',\n",
       " 'The expression represents a simultaneous adjustment of parameters \\\\( \\\\theta_i \\\\) in a function \\\\( J \\\\) with respect to the derivatives of \\\\( J \\\\) and a factor \\\\( s_i \\\\).',\n",
       " 'The provided input \"θi\" does not contain sufficient information to summarize.',\n",
       " 'The algorithm features an adaptive learning rate that decays faster for steep dimensions, facilitating more direct updates towards the global optimum and reducing the need for extensive tuning of the learning rate hyperparameter η.',\n",
       " 'The paper presents adaptive subgradient methods for online learning and stochastic optimization, focusing on their efficiency and effectiveness in handling large-scale problems. The authors introduce algorithms that adaptively adjust learning rates based on past gradients, ensuring improved convergence and performance in various applications.',\n",
       " \"AdaGrad is effective for simple quadratic problems but tends to stop early when training neural networks due to excessive scaling down of the learning rate, preventing it from reaching the global optimum. Although TensorFlow offers an AdagradOptimizer, it's not recommended for deep neural network training, while it may still work well for simpler tasks like Linear Regression.\",\n",
       " \"RMSProp improves on AdaGrad by accumulating only recent gradients using exponential decay, addressing AdaGrad's tendency to slow down too quickly and fail to converge to the global optimum.\",\n",
       " 'The reference \"11-7)\" does not provide sufficient context or content to summarize.',\n",
       " 'The RMSProp algorithm is an adaptive learning rate optimization technique that adjusts the learning rate for each parameter based on the average of recent magnitudes of the gradients. It aims to improve convergence by using a running average of squared gradients to normalize the learning rates, thereby preventing oscillations and ensuring more stable updates during training.',\n",
       " 'The decay rate β is usually set to 0.9, a default value that often performs well without requiring tuning.',\n",
       " 'TensorFlow includes a class called RMSPropOptimizer.',\n",
       " 'The code snippet initializes an RMSPropOptimizer in TensorFlow with a specified learning rate.',\n",
       " 'The parameters specified are momentum set to 0.9, decay set to 0.9, and epsilon set to 1e-10.',\n",
       " 'The optimizer outperforms AdaGrad, Momentum, and Nesterov Accelerated Gradients in most cases and was favored by researchers before the emergence of Adam optimization.',\n",
       " 'Adam (Adaptive Moment Estimation) integrates Momentum optimization and RMSProp by maintaining an exponentially decaying average of past gradients.',\n",
       " 'The provided text refers to downloading content from the website finelybook.com.',\n",
       " 'The text refers to a method for calculating an exponentially decaying average of past squared gradients, likely used in optimization algorithms.',\n",
       " 'The text \"11-8).16\" appears to reference a specific section or item, but without additional context or content, a summary cannot be provided.',\n",
       " 'Equation 11-8 describes the Adam optimization algorithm, which combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. It computes adaptive learning rates for each parameter from estimates of first and second moments of the gradients, allowing for efficient and effective training of deep learning models.',\n",
       " 'The text outlines components of an optimization algorithm, including terms for momentum (β1, β2), gradient calculations (∇θJ θ), and adjustments for learning rate (η) and bias correction (1−β1^T, 1−β2^T).',\n",
       " 'Adam optimization is similar to Momentum and RMSProp, differing only in step 1, which computes an exponentially decaying average instead of a sum. Steps 3 and 4 address the initial bias of m and s towards 0 by boosting their values at the start of training.',\n",
       " 'The momentum decay hyperparameter β1 is initialized to 0.9, β2 to 0.999, and the smoothing term ϵ to 10–8, which are the default values for TensorFlow’s AdamOptimizer class.',\n",
       " 'The code snippet initializes the Adam optimizer from TensorFlow with a specified learning rate.',\n",
       " 'Adam is an adaptive learning rate algorithm that simplifies the tuning of the learning rate hyperparameter, often using a default value of η = 0.001, making it easier to use than Gradient Descent.',\n",
       " 'Faster optimizers are advanced algorithms designed to improve the speed and efficiency of training machine learning models, reducing convergence time and enhancing overall performance.',\n",
       " 'No information provided to summarize.',\n",
       " '299',\n",
       " 'First-order optimization techniques using Jacobians are easier to apply than second-order techniques that rely on Hessians, which are difficult to use in deep neural networks due to the large number of parameters leading to memory and computational challenges.',\n",
       " 'The optimization algorithms discussed create dense models with many nonzero parameters, but for faster runtime and reduced memory usage, sparse models are preferable.',\n",
       " 'The method described involves training a model in the usual manner and then removing a small component.',\n",
       " 'Weights have been set to 0.',\n",
       " 'Applying strong ℓ1 regularization during training encourages the optimizer to eliminate as many weights as possible, similar to Lasso Regression.',\n",
       " 'Dual Averaging, or Follow The Regularized Leader (FTRL), is a technique by Yurii Nesterov that, when combined with ℓ1 regularization, produces sparse models. TensorFlow offers a version called FTRL-Proximal in the FTRLOptimizer class.',\n",
       " 'Finding the right learning rate is crucial; too high may cause divergence, too low results in slow convergence, and slightly high can lead to oscillation around the optimum. Adaptive learning rate algorithms like AdaGrad, RMSProp, or Adam can help but may still require time to stabilize.',\n",
       " 'Training before convergence may result in a suboptimal solution.',\n",
       " 'To find an optimal learning rate, train the network multiple times over a few epochs with different learning rates and compare the learning curves. The best learning rate will enable quick learning and convergence to a good solution.',\n",
       " 'Learning schedules are strategies used to reduce the learning rate during training, starting with a high rate and decreasing it when progress slows, potentially leading to faster convergence than using a constant learning rate.',\n",
       " 'The predetermined piecewise constant learning rate involves setting specific constant values for the learning rate at predefined intervals during training, allowing for controlled adjustments to optimize learning.',\n",
       " 'Set the initial learning rate to 0.1, then change it to 0.001 after 50 epochs, though finding the optimal learning rates can require experimentation.',\n",
       " 'Performance scheduling involves organizing and planning the timing and execution of various performances to ensure optimal coordination and resource management.',\n",
       " 'Validate the error every N steps and decrease the learning rate by a factor of λ when the error plateaus.',\n",
       " 'Exponential scheduling is a technique used in various fields, including computer science and project management, characterized by progressively increasing time intervals between tasks or events, often resulting in a rapid initial pace followed by a gradual slowdown. This approach can enhance efficiency and optimize resource allocation by prioritizing critical tasks while allowing for flexibility in scheduling.',\n",
       " 'The learning rate is defined as η(t) = η0 10–t/r, decreasing by a factor of 10 every r iterations, requiring tuning of η0 and r.',\n",
       " 'Power scheduling involves the allocation of electrical power resources to meet demand efficiently, ensuring reliability and minimizing costs. It includes strategies for load forecasting, generation planning, and demand response to optimize energy distribution.',\n",
       " 'The learning rate is defined as η(t) = η0 (1 + t/r)–c, with c usually set to 1, resulting in a slower decrease compared to exponential scheduling.',\n",
       " 'A 2013 paper by Andrew Senior et al. evaluated various learning schedules for training deep neural networks in speech recognition with Momentum optimization, finding that both performance and exponential scheduling were effective, but preferred exponential scheduling for its simplicity, ease of tuning, and slightly faster convergence to optimal solutions.',\n",
       " 'Implementing a learning schedule with TensorFlow is simple and easy to follow.',\n",
       " 'The initial learning rate is set to 0.1.',\n",
       " 'Decay steps are set to 10,000.',\n",
       " 'The decay rate is 0.1.',\n",
       " 'A TensorFlow variable named `global_step` is initialized to 0 and is not trainable.',\n",
       " \"The code snippet demonstrates the use of TensorFlow's `exponential_decay` function to adjust the learning rate based on the global step, with specified initial learning rate, decay steps, and decay rate.\",\n",
       " 'The code snippet defines a MomentumOptimizer with a learning rate and momentum of 0.9, and creates a training operation to minimize the loss while updating the global step.',\n",
       " \"The process involves setting hyperparameters, initializing a nontrainable variable `global_step` to track training iterations, defining an exponentially decaying learning rate using TensorFlow, creating a MomentumOptimizer with this learning rate, and finalizing the training operation by calling the optimizer's minimize method, which increments the `global_step`.\",\n",
       " 'AdaGrad, RMSProp, and Adam optimizations automatically reduce the learning rate, eliminating the need for an additional learning schedule, while other algorithms benefit from exponential decay or performance scheduling to enhance convergence speed.',\n",
       " 'The statement suggests that four parameters are sufficient to describe an elephant, while five parameters allow for a more dynamic representation, enabling the elephant to \"wiggle.\"',\n",
       " 'Trunk refers to the main stem of a tree, supporting branches and leaves, and serving as a conduit for nutrients and water.',\n",
       " 'John von Neumann is referenced by Enrico Fermi in the publication Nature 427.',\n",
       " 'Deep neural networks have a large number of parameters, allowing them to fit complex datasets, but this flexibility increases the risk of overfitting the training data.',\n",
       " 'The section discusses popular regularization techniques for neural networks, including early stopping, ℓ1 and ℓ2 regularization, dropout, max-norm regularization, and data augmentation, along with their implementation in TensorFlow.',\n",
       " 'Early stopping is a technique used to prevent overfitting by halting training when performance on the validation set begins to decline.',\n",
       " 'Implement a model evaluation with TensorFlow by assessing it on a validation set at regular intervals, saving the best-performing snapshots, counting steps since the last save, and stopping training when a set limit is reached, followed by restoring the last best snapshot.',\n",
       " 'Early stopping is effective, but combining it with other regularization techniques can yield higher network performance.',\n",
       " 'ℓ1 and ℓ2 regularization can be applied to constrain the connection weights of a neural network, similar to their use in simple linear models.',\n",
       " 'The provided text is incomplete and does not contain sufficient information to summarize.',\n",
       " 'Regularization in TensorFlow can be implemented by adding regularization terms to the cost function, particularly for models with a hidden layer and an output layer.',\n",
       " 'Apply ℓ1 regularization to the model.',\n",
       " 'The table or text outlines the steps and components involved in constructing a neural network, including defining the architecture, selecting activation functions, initializing weights, and training the model with data.',\n",
       " 'The code calculates the total loss for a model by averaging the cross-entropy loss (`base_loss`) and adding a regularization term (`reg_losses`), which is the sum of the absolute values of two sets of weights. The regularization term is scaled by a factor (`scale`).',\n",
       " 'TensorFlow offers an improved method for managing multiple layers by allowing the use of a *_regularizer argument in variable creation functions like get_variable() and fully_connected(). This enables the passing of functions that compute regularization loss for weights, utilizing l1_regularizer(), l2_regularizer(), and l1_l2_regularizer() for this purpose.',\n",
       " 'The provided text appears to be a code snippet related to the use of an argument scope in programming, likely for configuring functions or methods with specific parameters.',\n",
       " 'The table \"fully_connected\" is not provided, so I cannot summarize its content. Please provide the table data for summarization.',\n",
       " 'The code snippet implements an L1 regularizer with a scale of 0.01 and defines two fully connected hidden layers in a neural network, named \"hidden1\" and \"hidden2\", with specified hidden unit counts.',\n",
       " 'The expression defines a fully connected layer named \"out\" that connects the second hidden layer (`hidden2`) to the output layer with no activation function applied, producing logits for `n_outputs`.',\n",
       " \"The code establishes a neural network with two hidden layers and one output layer, incorporating nodes for ℓ1 regularization loss for each layer's weights, which TensorFlow automatically collects. These regularization losses should be included in the overall loss calculation.\",\n",
       " 'The code retrieves regularization losses from a TensorFlow collection and adds them to a base loss to compute the total loss.',\n",
       " 'Regularization losses must be included in the overall loss to ensure they are accounted for.',\n",
       " 'Dropout, proposed by G. E. Hinton in 2012 and elaborated by Nitish Srivastava et al., is a highly effective regularization technique for deep neural networks, providing a 1-2% accuracy improvement, which significantly lowers the error rate for models with already high accuracy.',\n",
       " 'The algorithm involves temporarily \"dropping out\" neurons during training steps, where each neuron (except output neurons) has a probability p of being ignored, typically set to 50%. After training, all neurons are active.',\n",
       " 'The text discusses the surprising effectiveness of a technique similar to random employee attendance in companies, suggesting that it could lead to a more resilient organization where expertise is shared among employees. This concept parallels the dropout method in neural networks, where neurons must function independently and rely on all input neurons, resulting in a more robust network that generalizes better.',\n",
       " 'The input is incomplete and does not provide any table or text to summarize. Please provide the relevant content for the summary.',\n",
       " 'Dropout generates a unique neural network at each training step by randomly including or excluding neurons, resulting in 2N possible networks for N dropable neurons. After 10,000 training steps, approximately 10,000 distinct networks are trained, which, while not independent due to shared weights, create an averaging ensemble of smaller networks.',\n",
       " 'During testing, a neuron will connect to twice as many input neurons as it did on average during training when p = 50, necessitating a compensation by multiplying each neuron.',\n",
       " \"After training, input connection weights should be multiplied by the keep probability (1 - p) to prevent neurons from receiving an input signal twice as large as during training. Alternatively, dividing each neuron's output by the keep probability during training can also be used, though the two methods are not perfectly equivalent.\",\n",
       " 'Dropout in TensorFlow is implemented using the dropout() function on input and hidden layers, randomly setting some items to 0 during training and scaling the remaining items by the keep probability. After training, the function has no effect.',\n",
       " 'The statement imports the `dropout` function from the `tensorflow.contrib.layers` module in TensorFlow.',\n",
       " 'Please provide the table or text chunk you would like summarized.',\n",
       " \"The code defines a TensorFlow placeholder named 'is_training' that holds a boolean value, indicating whether the model is in training mode.\",\n",
       " 'The keep probability is set at 0.5.',\n",
       " 'The function `dropout` applies dropout regularization to the input tensor `X`, retaining a fraction of the input data determined by `keep_prob`, and operates in training mode if `is_training` is set to true.',\n",
       " 'The code snippet defines a fully connected layer named \"hidden1\" using the dropout technique to prevent overfitting during training. It applies a dropout layer to \"hidden1\" with a specified probability of keeping neurons active, depending on the training state.',\n",
       " 'The code defines a fully connected layer named \"hidden2\" that takes the output from \"hidden1_drop\" and has \"n_hidden2\" units. It then applies dropout to \"hidden2\" using a probability \"keep_prob\" during training.',\n",
       " 'The code snippet defines a fully connected layer named \"outputs\" that takes input from \"hidden2_drop\" and produces output with a specified number of outputs, using no activation function.',\n",
       " \"Use the dropout() function from tensorflow.contrib.layers, as it disables during non-training phases, unlike tensorflow.nn's function.\",\n",
       " 'Set is_training to True during training and to False during testing.',\n",
       " 'To address overfitting, increase the dropout rate (decrease keep_prob), while for underfitting, decrease the dropout rate (increase keep_prob). Larger layers may benefit from higher dropout rates, whereas smaller layers may require lower rates.',\n",
       " 'Dropout slows down convergence but can lead to a significantly better model when properly tuned, making it worthwhile.',\n",
       " 'Dropconnect is a variant of dropout that randomly drops individual connections instead of entire neurons, with dropout generally showing better performance.',\n",
       " 'Max-norm regularization is a technique for neural networks that constrains the weights of incoming connections for each neuron, ensuring that the ℓ2 norm of the weights does not exceed a specified hyperparameter r.',\n",
       " 'The constraint is implemented by calculating the L2 norm of the weight vector \\\\( w \\\\) after each training step and clipping \\\\( w \\\\) if necessary.',\n",
       " \"Please provide the table or text chunk you'd like summarized.\",\n",
       " 'Reducing r increases regularization, which helps mitigate overfitting, while max-norm regularization can address vanishing/exploding gradient issues if Batch Normalization is not used.',\n",
       " 'TensorFlow lacks a built-in max-norm regularizer, but it can be implemented by creating a node, `clip_weights`, to clip the weights variable along the second axis, ensuring each row vector has a maximum norm.',\n",
       " 'The input is incomplete; please provide the table or text chunk to be summarized.',\n",
       " 'The threshold is set to 1.0.',\n",
       " 'The code snippet clips the weights tensor to a specified norm threshold and then assigns the clipped values back to the weights tensor.',\n",
       " 'The operation is to be applied after each training step.',\n",
       " 'The code snippet demonstrates the usage of a TensorFlow session to train a model over a specified number of epochs, iterating through batches of input data (X_batches) and corresponding labels (y_batches) to perform training operations and evaluate weight clipping.',\n",
       " 'To access the weights variable of each layer, use a variable scope.',\n",
       " 'The code snippet defines a fully connected neural network layer named \"hidden1\" with input data `X` and `n_hidden1` neurons.',\n",
       " 'The code snippet defines a TensorFlow variable scope named \"hidden1\" in reuse mode and retrieves a variable called \"weights\" within that scope.',\n",
       " 'The provided text does not contain any specific information or data to summarize.',\n",
       " 'The code defines two fully connected layers in a neural network: the first layer connects input X to a hidden layer with n_hidden1 units, and the second layer connects the first hidden layer to another hidden layer with n_hidden2 units.',\n",
       " 'The content cannot be accessed as it requires a download from an external website.',\n",
       " 'Please provide the table or text chunk you would like summarized.',\n",
       " 'To identify variable names, use TensorBoard or the global_variables() function to print all variable names.',\n",
       " 'The code iterates through all global variables in TensorFlow and prints their names.',\n",
       " 'A cleaner approach involves creating a `max_norm_regularizer()` function to simplify the process, similar to the `l1_regularizer()` function.',\n",
       " 'The function `max_norm_regularizer` applies a maximum norm regularization technique to constrain the weights of a model. It takes parameters such as `threshold` for the maximum norm value, `axes` to define the dimensions over which to apply the regularization, and an optional `name` for identifying the regularizer.',\n",
       " 'The code defines a max-norm regularizer function that clips weights to a specified threshold using TensorFlow. It includes parameters for axes, a name for the operation, and a collection name for managing the operation. The function returns `None` as there is no regularization loss term.',\n",
       " 'The function `max_norm(weights)` is likely intended to compute the maximum norm of a given set of weights, although the specific implementation details are not provided in the snippet.',\n",
       " 'The function provides a parameterized max_norm() regularizer for use in various applications.',\n",
       " 'The code defines a max norm regularizer with a threshold of 1.0 and applies it to the weights of a fully connected layer named \"hidden1\" with a specified number of hidden units (n_hidden1).',\n",
       " 'Max-norm regularization does not add a loss term, and the max_norm() function returns None. However, it adds a clip_weights node for managing weight clipping after each training step, which must be executed subsequently.',\n",
       " 'The variable `clip_all_weights` is assigned the result of retrieving the collection named \"max_norm\" using TensorFlow\\'s `tf.get_collection` method.',\n",
       " \"The code snippet demonstrates the use of TensorFlow's session management to train a model over multiple epochs. Within each epoch, it iterates through batches of training data, executing the training operation and clipping the weights.\",\n",
       " 'Cleaner code is achieved.',\n",
       " 'Unable to access external websites or content. Please provide the table or text for summarization.',\n",
       " 'Data augmentation is a regularization technique that generates new training instances from existing ones to increase the training set size and reduce overfitting. The generated instances should be realistic, making it difficult for humans to distinguish them from original instances. Simple modifications like adding white noise are insufficient; the changes must be learnable.',\n",
       " 'Data augmentation techniques, such as shifting, rotating, resizing, changing contrast, and flipping images, can enhance the training set for mushroom classification models, improving their tolerance to variations in position, orientation, size, and lighting conditions.',\n",
       " 'Generating training instances on the fly during training is preferable to saving storage and bandwidth. TensorFlow provides various image manipulation operations, including transposing, rotating, resizing, flipping, cropping, and adjusting brightness, contrast, saturation, and hue.',\n",
       " 'The API documentation facilitates easy implementation of data augmentation for image datasets.',\n",
       " 'Adding skip connections, which involve combining the input of a layer with the output of a higher layer, is an effective technique for training very deep neural networks, to be further explored in Chapter 13 regarding deep residual networks.',\n",
       " 'The chapter discusses various techniques and suggests that the configuration in Table 11-2 is suitable for most situations.',\n",
       " 'Table 11-2 outlines the default configuration settings for a Deep Neural Network (DNN), including parameters such as layer types, activation functions, learning rates, and optimization algorithms.',\n",
       " 'Initialization refers to the process of setting up or preparing a system, application, or program to ensure it is ready for operation.',\n",
       " 'An activation function is a mathematical operation used in neural networks to introduce non-linearity into the model, allowing it to learn complex patterns. Common types include Sigmoid, Tanh, ReLU, and Softmax, each with different properties and use cases.',\n",
       " 'Normalization is a process in data management that involves organizing data to minimize redundancy and improve data integrity. It typically includes structuring a database into tables and defining relationships between them to ensure efficient data retrieval and manipulation.',\n",
       " 'Batch Normalization is a technique used in deep learning to normalize the inputs of each layer in a neural network, which helps to stabilize and accelerate the training process. It reduces internal covariate shift, allows for higher learning rates, and can act as a regularizer, potentially reducing the need for dropout. The method involves scaling and shifting the normalized outputs using learned parameters.',\n",
       " 'Regularization is a technique used in machine learning to prevent overfitting by introducing additional information or constraints into the model. It typically involves adding a penalty term to the loss function, which discourages overly complex models. Common regularization methods include Lasso (L1) and Ridge (L2) regularization, each affecting model coefficients differently to promote generalization and improve predictive performance.',\n",
       " 'The term \"Optimizer\" refers to a method or algorithm used to improve the performance or efficiency of a system, often in the context of machine learning or mathematical functions.',\n",
       " 'A learning rate schedule is a strategy for adjusting the learning rate during training to improve model performance and convergence. It typically involves decreasing the learning rate over time or based on certain criteria, helping to stabilize training and enhance final accuracy.',\n",
       " 'No content to summarize.',\n",
       " 'Reuse parts of a pretrained neural network when possible, especially if it addresses a similar problem.',\n",
       " 'The default configuration may require adjustments.',\n",
       " 'Training very deep networks may require patience, especially when using a single machine, which could lead to long wait times.',\n",
       " 'The next chapter will cover using distributed TensorFlow for training and running models on multiple servers and GPUs.',\n",
       " 'Three ways to produce a sparse model are: 1) Feature selection, 2) Regularization techniques (such as Lasso), and 3) Pruning methods.',\n",
       " 'The content suggests attempting again and inquires if it is helpful.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize text\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n",
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d76024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table lists methods along with a numerical value of 2 for each: \"rgistic,\" \"yperbolic tangent,\" and \"LU (and its variants).\" Additionally, there are formulas related to inputs and outputs associated with these methods.',\n",
       " 'The table outlines the following settings for a neural network: He initialization with ELU activation function, batch normalization, dropout for regularization, Adam optimizer, and no learning rate schedule.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize tables\n",
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})\n",
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a graph representing the Leaky ReLU (Rectified Linear Unit) activation function, commonly used in neural networks. \n",
      "\n",
      "### Key Features of the Graph:\n",
      "\n",
      "1. **Axes**: \n",
      "   - The horizontal axis (x-axis) ranges from approximately -4 to 4, indicating the input values to the activation function.\n",
      "   - The vertical axis (y-axis) ranges from 0 to 4, representing the output values of the Leaky ReLU function.\n",
      "\n",
      "2. **Function Behavior**:\n",
      "   - For negative input values (from -4 to 0), the graph shows a line with a slight upward slope, indicating that the function allows a small, non-zero output (the \"leak\") for negative inputs. This contrasts with the standard ReLU, which outputs zero for negative values.\n",
      "   - At the point of zero input (0 on the x-axis), there is a transition in the slope.\n",
      "   - For positive input values (from 0 to 4), the graph exhibits a straight line with a slope of 1, indicating that the output equals the input.\n",
      "\n",
      "3. **Labeling**:\n",
      "   - The graph is titled \"Leaky ReLU activation function\" at the top.\n",
      "   - An arrow labeled \"Leak\" points to the region where the function produces a small positive value for negative inputs, further emphasizing the unique feature of this activation function.\n",
      "\n",
      "4. **Color and Style**:\n",
      "   - The line representing the function is colored blue, standing out against the white background, which helps in visual clarity.\n",
      "   - The grid lines are faint, providing a reference for reading values without overwhelming the graph.\n",
      "\n",
      "Overall, this illustration effectively conveys the behavior of the Leaky ReLU activation function, highlighting its utility in addressing the \"dying ReLU\" problem by maintaining a small output for negative inputs.\n"
     ]
    }
   ],
   "source": [
    "# Summarize images\n",
    "\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of a research paper explaining the transformers\n",
    "                  architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "image_summaries = chain.batch(images)\n",
    "print(image_summaries[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\002PMB744\\AppData\\Local\\Temp\\ipykernel_23176\\3922053165.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n",
      "C:\\Users\\002PMB744\\AppData\\Local\\Temp\\ipykernel_23176\\3922053165.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5094910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "# print(summary_texts[2].page_content,summary_texts[2].metadata)\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb576e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Xavier initialization strategy can speed up training considerably, and it is one of the tricks that led to the current success of Deep Learning. Some recent papers4 have provided similar strategies for different activation functions, as shown in Table 11-1. The initialization strategy for the ReLU activation function (and its var‐ iants, including the ELU activation described shortly) is sometimes called He initiali‐ zation (after the last name of its author).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Equation 11-1. Xavier initialization (when using the logistic activation function)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "By default, the fully_connected() function (introduced in Chapter 10) uses Xavier initialization (with a uniform distribution). You can change this to He initialization by using the variance_scaling_initializer() function like this:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "In their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐ lem. We need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradi‐ ents. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs,2 and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). It is actually not possible to guarantee both unless the layer has an equal number of input and output connections, but they proposed a good compromise that has proven to work very well in practice: the connection weights must be initialized randomly as described in Equation 11-1, where ninputs and noutputs are the number of input and output connections for the layer whose weights are being initialized (also called fan-in and fan-out). This initialization strategy is often called Xavier initializa‐ tion (after the author’s first name), or sometimes Glorot initialization.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve\n",
    "docs = retriever.invoke(\n",
    "    \"who is Xavier and He Initialization?\"\n",
    ")\n",
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6fc8f",
   "metadata": {},
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d90a59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base64 import b64decode\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.text\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39b59179",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"What is Xavier initialization?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e536e47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xavier initialization, also known as Glorot initialization, is a strategy for initializing the weights of layers in a neural network. It aims to improve the flow of signals during both forward predictions and backward gradient propagation. According to the authors Glorot and Bengio, this initialization helps maintain equal variance of the outputs and gradients before and after passing through a layer. It requires that the weights be initialized randomly based on the number of input connections (fan-in) and output connections (fan-out) for the layer. This approach alleviates issues related to signal dying out or exploding, facilitating more effective training of deep learning models.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
